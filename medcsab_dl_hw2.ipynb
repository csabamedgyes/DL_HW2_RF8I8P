{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP1QfEkIIljIFIM9UkptWb8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/csabamedgyes/DL_HW2_RF8I8P/blob/main/medcsab_dl_hw2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Introduction\n",
        "My name is Csaba Medgyes (Neptun code: RF8I8P) and this is my second Homework for the Deep Learning class. Our excercies are:\n",
        "\n",
        "In lab we constructed a neural network in Numpy. Construct the mini-batch based learning (with forward and backward steps). \n",
        "\n",
        "Upload it into a secret gist in github with the option of opening in colab!\n",
        "\n",
        "From what I did different from the lab I am going to denote by the following so it is easier to review:"
      ],
      "metadata": {
        "id": "_nJleQNO-Urv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#MODIFICATION START\n",
        "\n",
        "##############################################################################################################################################################\n",
        "\n",
        "    #lorem ipsum code\n",
        "\n",
        "##############################################################################################################################################################\n",
        "\n",
        "#MODIFICATION END"
      ],
      "metadata": {
        "id": "uwpdo7NbJwX6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import the necessary packages for later updates."
      ],
      "metadata": {
        "id": "tqeVK4kLJghR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "%matplotlib inline \n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import preprocessing"
      ],
      "metadata": {
        "id": "bdpmQCmqAm68"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The given code from lab with modification between comment lines for mini batch based learning"
      ],
      "metadata": {
        "id": "JrdOeiEL_gKx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iz-d3UKy-SPy"
      },
      "outputs": [],
      "source": [
        "# activációs függvény\n",
        "\n",
        "def activation(x):\n",
        "\n",
        "  return 1.0/(1.0 + np.exp(-x))\n",
        "\n",
        "# derivative of the activation function\n",
        "\n",
        "def dactivation(x):\n",
        "    return np.exp(-x) / (1 + np.exp(-x))**2\n",
        "\n",
        "class MLP:\n",
        "    \n",
        "    def __init__(self, *args):\n",
        "        \"\"\"\n",
        "        A hálózat inicializálása az argumentumként megadott méretek alapján.\n",
        "        \"\"\"\n",
        "        # random seed megadása\n",
        "        np.random.seed(123)\n",
        "        # A hálózat formája (rétegek száma), amely megegyezik a paraméterek számával\n",
        "        self.shape = args\n",
        "        n = len(args)\n",
        "        # Rétegek létrehozása\n",
        "        self.layers = []\n",
        "        # Bemeneti réteg létrehozása (+1 egység a BIAS-nak)\n",
        "        self.layers.append(np.ones(self.shape[0]+1))\n",
        "        # Rejtett réteg(ek) és a kimeneti réteg létrehozása\n",
        "        for i in range(1,n):\n",
        "            self.layers.append(np.ones(self.shape[i]))\n",
        "        # Súlymátrix létrehozása\n",
        "        self.weights = []\n",
        "        for i in range(n-1):\n",
        "            self.weights.append(np.zeros((self.layers[i].size,\n",
        "                                         self.layers[i+1].size)))\n",
        "        # dw fogja tartalmazni a súlyok utolsó módosításait (később pl. a momentum módszer számára)\n",
        "        self.dw = [0,]*len(self.weights)\n",
        "        # Súlyok újrainicializálása\n",
        "        self.reset()\n",
        "    \n",
        "    def reset(self):\n",
        "        \"\"\"\n",
        "        Súlyok újrainicializálása [-1, 1) intervallum fölött vett egyenletes eloszlás mintavételezésével\n",
        "        \"\"\"\n",
        "        for i in range(len(self.weights)):\n",
        "            # véletlen számok [0,1) tartományban \n",
        "            Z = np.random.random((self.layers[i].size,self.layers[i+1].size))\n",
        "            # átskálázzuk a súlyokat [-1,1) tartományba\n",
        "            self.weights[i][...] = (2*Z-1)*1\n",
        "\n",
        "    def propagate_forward(self, data):\n",
        "        \"\"\"\n",
        "        A bemenő adatok végigküldése a hálózaton, kimeneti rétegig (forward propagation)\n",
        "        \"\"\"\n",
        "        # Bemeneti réteg beállítása (tanító adatok)\n",
        "        self.layers[0][0:-1] = data\n",
        "        # Az adatok végigküldése a bemeneti rétegtől az utolsó előtti rétegig (az utolsó ugyanis a kimeneti réteg).\n",
        "        # A szigmoid aktivációs függvény használatával, mátrixszorzások alkalmazásával.\n",
        "        # Az előadáson a \"layers\" változót jelöltük \"a\"-val.\n",
        "        for i in range(1,len(self.shape)):\n",
        "            s_i = np.dot(self.layers[i-1], self.weights[i-1])\n",
        "            self.layers[i][...] = activation(s_i)\n",
        "        # Visszatérés a hálózat által becsült eredménnyel\n",
        "        return self.layers[-1]\n",
        "\n",
        "    def propagate_backward(self, target, lrate=0.1):\n",
        "        \"\"\"\n",
        "        Hibavisszaterjesztés (backpropagation) definiálása. \n",
        "        Args:\n",
        "            target: címkék, a kimenetek elvárt értékei\n",
        "            lrate: \n",
        "                A a learning rate (tanulási ráta) paraméter befolyásolja, hogy a hálózat súlyait milyen\n",
        "                mértékben módosítsuk a gradiens függvényében. Ha ez az érték túl magas, akkor a háló \n",
        "                \"oszcillálhat\" egy lokális vagy globális minimum körül. Ha túl kicsi értéket választunk,\n",
        "                akkor pedig jelentősen több időbe telik mire elérjük a legjobb megoldást vagy leakad egy \n",
        "                lokális minimumban és sose éri el azt. Defaults to 0.1.\n",
        "\n",
        "        Returns: az aktuális kimenetek és elvárt értékek alapján számított (négyzetes) hiba\n",
        "        \"\"\"\n",
        "        deltas = []\n",
        "        # Hiba: 1/2 (y-y_kalap)**2\n",
        "        # Hiba deriváltjának kiszámítása a kimeneti rétegen (dC/dy_kalap)\n",
        "        derror = -(target-self.layers[-1]) # y-y_kalap\n",
        "        # error*dactivation(s(3))\n",
        "        s_last = np.dot(self.layers[-2],self.weights[-1])\n",
        "        delta_last = derror * dactivation(s_last)\n",
        "        deltas.append(delta_last)\n",
        "        # Gradiens kiszámítása a rejtett réteg(ek)ben\n",
        "        for i in range(len(self.shape)-2,0,-1):\n",
        "            s_i = np.dot(self.layers[i-1],self.weights[i-1])\n",
        "            # pl. utolsó rejtett réteg: delta(3)*(W(2).T)*dactivation(s(2)) (lásd előadás)\n",
        "            delta_i = np.dot(deltas[0],self.weights[i].T)*dactivation(s_i)\n",
        "            # a háló eleje felé \"lépkedünk, mindig a deltas tömb elejére szúrjuk be az aktuálisan kiszámítottat\"\n",
        "            deltas.insert(0,delta_i)            \n",
        "        # Súlyok módosítása\n",
        "        for i in range(len(self.weights)):\n",
        "            layer = np.atleast_2d(self.layers[i])\n",
        "            delta = np.atleast_2d(deltas[i])\n",
        "            # pl. utolsó rétegben: delta(3)*a(2) (lásd előadás)\n",
        "            dw = -lrate*np.dot(layer.T,delta)\n",
        "            # súlyok módosítása\n",
        "            self.weights[i] += dw \n",
        "\n",
        "            # a súlymódosítás eltárolása\n",
        "            self.dw[i] = dw\n",
        "\n",
        "        # Visszatérés a hibával\n",
        "        error = (target-self.layers[-1])**2 \n",
        "        return error.sum()\n",
        "\n",
        "    #MODIFICATION START\n",
        "\n",
        "    ##############################################################################################################################################################\n",
        "\n",
        "    def updating_weights(self, dws, batch_size=32):\n",
        "\n",
        "        #We should update the weights with dws - the difference of weights\n",
        "\n",
        "        for l in range(len(self.weights)):\n",
        "\n",
        "            #We should divide by batch_size because we want to get the average of gradients (not the sum)\n",
        "\n",
        "            dws[l] /= batch_size\n",
        "\n",
        "            self.weights[l] += dws[l]\n",
        "            \n",
        "    def returning_self_weights(self):\n",
        "        return self.weights\n",
        "    \n",
        "    def returning_self_dw(self):\n",
        "        return self.dw\n",
        "\n",
        "    ###############################################################################################################################################################\n",
        "\n",
        "    #MODIFICATION END"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def learn(network, X, Y, valid_split, test_split, writer, epochs=20, lrate=0.1, batch_size = 32):\n",
        "\n",
        "        # train-validation-test minták különválasztása\n",
        "        nb_samples = len(Y)\n",
        "        X_train = X[0:int(nb_samples*(1-valid_split-test_split))]\n",
        "        Y_train = Y[0:int(nb_samples*(1-valid_split-test_split))]\n",
        "        X_valid = X[int(nb_samples*(1-valid_split-test_split)):int(nb_samples*(1-test_split))]\n",
        "        Y_valid = Y[int(nb_samples*(1-valid_split-test_split)):int(nb_samples*(1-test_split))]\n",
        "        X_test  = X[int(nb_samples*(1-test_split)):]\n",
        "        Y_test  = Y[int(nb_samples*(1-test_split)):]\n",
        "    \n",
        "        # standardizálás\n",
        "        scaler = preprocessing.StandardScaler().fit(X_train)\n",
        "        X_train = scaler.transform(X_train)\n",
        "        X_valid = scaler.transform(X_valid)\n",
        "        X_test  = scaler.transform(X_test)\n",
        "    \n",
        "        # ugyanolyan sorrendben keverjük be a bemeneteket és kimeneteket, a három külön adatbázisra\n",
        "        randperm = np.random.permutation(len(X_train))\n",
        "        X_train, Y_train = X_train[randperm], Y_train[randperm]\n",
        "        \n",
        "#MODIFICATION START\n",
        "\n",
        "##############################################################################################################################################################\n",
        "\n",
        "      # In the training phase, we have to go through every batch in every epoch:\n",
        "\n",
        "        for i in range(epochs):\n",
        "\n",
        "            number_of_batches = X_train.shape[0] // batch_size\n",
        "\n",
        "            # this is the training error in an epoch\n",
        "            train_error_all = 0\n",
        "\n",
        "            for j in range(number_of_batches):\n",
        "\n",
        "                # training error during a batch\n",
        "                train_error = 0\n",
        "\n",
        "                # the difference of the weights:\n",
        "\n",
        "                dws = np.zeros_like(network.returning_self_weights())\n",
        "\n",
        "                # we have to check every sample of the batch\n",
        "\n",
        "                for k in range(j * batch_size,(j+1) * batch_size):\n",
        "\n",
        "                    network.propagate_forward( X_train[k] )\n",
        "\n",
        "                    train_error += network.propagate_backward( Y_train[k], lrate )\n",
        "\n",
        "                    #We should add the difference of weights of every sample to the whole difference of weights\n",
        "\n",
        "                    for l in range(len(network.returning_self_weights())):\n",
        "\n",
        "                        dws[l] += network.returning_self_dw()[l]\n",
        "\n",
        "                network.updating_weights(dws, batch_size)\n",
        "\n",
        "                train_error /= batch_size\n",
        "                train_error_all += train_error\n",
        "\n",
        "            train_error_all /= number_of_batches\n",
        "\n",
        "##############################################################################################################################################################\n",
        "\n",
        "#MODIFICATION END\n",
        "\n",
        "            # validation phase (Same as in notebook)\n",
        "            valid_err = 0\n",
        "            o_valid = np.zeros(X_valid.shape[0])\n",
        "            for k in range(X_valid.shape[0]):\n",
        "                o_valid[k] = network.propagate_forward(X_valid[k])\n",
        "                valid_err += (o_valid[k]-Y_valid[k])**2\n",
        "            valid_err /= X_valid.shape[0]\n",
        "\n",
        "            print(\"%d epoch, train_error: %.4f, valid_err: %.4f\" % (i, train_error_all, valid_err))\n",
        "\n",
        "        # Tesztelési fázis\n",
        "        print(\"\\n--- TESZTELÉS ---\\n\")\n",
        "        test_err = 0\n",
        "        o_test = np.zeros(X_test.shape[0])\n",
        "        for k in range(X_test.shape[0]):\n",
        "            o_test[k] = network.propagate_forward(X_test[k])\n",
        "            test_err += (o_test[k]-Y_test[k])**2\n",
        "            print(k, X_test[k], '%.2f' % o_test[k], ' (elvart eredmeny: %.2f)' % Y_test[k])\n",
        "        test_err /= X_test.shape[0]\n",
        "\n",
        "        plt.scatter(X_test[:,0], X_test[:,1], c=np.round(o_test[:]), cmap=plt.cm.coolwarm)"
      ],
      "metadata": {
        "id": "ZuIknPKJIdap"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}